# A High-performing Mid-range NAS Server
## Part 1: Initial Set-up and Testing

This blog post describes how we built a high-performing NAS server using off-the-shelf components and open source software ([FreeNAS](http://www.freenas.org/)). The NAS has the following characteristics:

* total cost (before tax & shipping): **$2,631**
* total usable storage: **16.6 TiB**
* cost / usable GiB: **$0.16/GiB**
* [IOPS](http://en.wikipedia.org/wiki/IOPS): **884** <sup>[[1]](#cpu_bound)</sup>
* sequential read: **1882MB/s** <sup>[[1]](#cpu_bound)</sup>
* sequential write: **993MB/s** <sup>[[1]](#cpu_bound)</sup>
* double-parity RAID: **RAID-Z2**

***[2014-10-31 We have updated the performance numbers. The old numbers were wrong (they were too low). Specifically, the old performance numbers were generated by a benchmark which was CPU-bound, not disk-bound. We re-generated the numbers by running 8 benchmarks in parallel and aggregating the results]***

[caption id="attachment_30838" align="alignnone" width="630"]<a href="http://pivotallabs.com/wordpress/wp-content/uploads/2014/10/freenas_pellegrino.jpg"><img src="http://pivotallabs.com/wordpress/wp-content/uploads/2014/10/freenas_pellegrino-630x566.jpg" alt="Our NAS server: 28TB raw data. 1 liter Pellegrino bottle is for scale" width="630" height="566" class="size-large wp-image-30838" /></a> Our NAS server: 28TB raw data. 1 liter Pellegrino bottle is for scale[/caption]

***[Author's note &amp; disclosure: This FreeNAS server is a personal project, intended for my home lab. At work we use an EMC VNX5400, with which we are quite pleased&mdash;in fact we are planning to triple its storage. I am employed by Pivotal Labs, which is partly-owned by EMC, a storage manufacturer]***

### 1. The Equipment
Prices do not include tax and shipping. Prices were current as of September, 2014.

* **$375**: 1 &times; [Supermicro A1SAi-2750F Mini-ITX 20W 8-Core Intel C2750 motherboard](http://www.supermicro.com/products/motherboard/Atom/X10/A1SAi-2750F.cfm). This motherboard *includes* the Intel 8-core Atom C2750F. We chose this particular motherboard and chipset combination for its mini-ITX form factor (space is at a premium in our setup) and its low TDP (thermal design power) (better for the environment, lower electricity bills, heat-dissipation not a concern). The low TDP allows for passive CPU cooling (i.e. no CPU fan).
* **$372**: 4 &times; [Kingston KVR13LSE9/8 8GB ECC SODIMM](http://www.kingston.com/datasheets/KVR13LSE9_8.pdf). 32GiB is a good match for our aggregate drive size (28TB), for ZFS allocates approximately 1GiB RAM for every 1TB raw storage (i.e. we use 28GiB used for ZFS, leaving 4GiB for the operating system and L2ARC).
* **$1,190**: 7 &times; [Seagate 4TB NAS HDD ST4000VN000](http://www.seagate.com/internal-hard-drives/nas-drives/nas-hdd/?sku=ST4000VN000). According to [Calomel.org](https://calomel.org/zfs_raid_speed_capacity.html), 'You do not have the get the most expensive SAS drives &hellip; Look for any manufacture [*sic*] which label their drives as RE or Raid Enabled or "For NAS systems."' Calomel.org goes on to warn against using power saving or ECO mode drives, for they have poor RAID performance.
* **$238**: 1 &times; [LSI SAS 9211-8i 6Gb/s SAS Host Bus Adapter](http://www.lsi.com/products/host-bus-adapters/pages/lsi-sas-9211-8i.aspx). Although expensive SAS drives don't offer much value over less-expensive Raid Enabled SATA drives, the SAS/SATA controller makes a big difference, at times more than three-fold. Calomel.org's section "[All SATA controllers are NOT created equal](https://calomel.org/zfs_raid_speed_capacity.html)" has a convincing series of benchmarks.
* **$215**: 1 &times; [Crucial MX100 512GB SSD](http://www.crucial.com/usa/en/ct512mx100ssd1). This drive is intended as a caching drive (ZFS's [L2ARC](https://blogs.oracle.com/brendan/entry/test) for reads ZFS's ZIL's [SLOG](https://pthree.org/2012/12/06/zfs-administration-part-iii-the-zfs-intent-log/) for writes). We chose this drive in particular because it offered power-loss protection (important for synchronous writes); however, Anandtech pointed out in the section "[The Truth About Micron's Power-Loss Protection](http://www.anandtech.com/show/8528/micron-m600-128gb-256gb-1tb-ssd-review-nda-placeholder))" that "&hellip; the M500, M550 and MX100 do not have power-loss protection&mdash;what they have is circuitry that protects against corruption of existing data in the case of a power-loss." We encourage the research of alternative SSDs if power-loss protection is desired.
* **$110**: 1 &times; [Corsair HX650 650 watt power supply](http://www.corsair.com/en-us/hx-series-hx650-power-supply-650-watt-80-plus-gold-certified-modular-psu). We believe we over-specified the wattage required for the power supply. [This poster](http://lime-technology.com/forum/index.php?topic=29670.0) recommends the [Silverstone ST45SF-G](http://www.silverstonetek.com/product.php?pid=342), whose size is half that of a standard ATX power supply, making it well-suited for our chassis.
* **$100**: 1 &times; [Lian Li PC-Q25B Mini ITX chassis](http://www.lian-li.com/en/dt_portfolio/pc-q25/). We like this chassis because in spite of its small form factor we are able to install 7 &times; 3.5" drives, 1 &times; 2.5" SSD, and the LSI controller.
* **$31**: 2 &times; [HighPoint SF-8087 &rarr; 4 &times; SATA cables](http://highpoint-tech.com/USA_new/accessories_int_ms1m4s.htm). Although we used a different manufacturer, these cables should work.


### 2. Assembly
[caption id="attachment_30839" align="alignnone" width="630"]<a href="http://pivotallabs.com/wordpress/wp-content/uploads/2014/10/freenas_inside.jpg"><img src="http://pivotallabs.com/wordpress/wp-content/uploads/2014/10/freenas_inside-630x472.jpg" alt="The inside view of the NAS. Note the interesting 3.5&quot; drive layout: 5 of them are in a column, and the remaining 2 (along with the SSD) are installed near the LSI controller" width="630" height="472" class="size-large wp-image-30839" /></a> The inside view of the NAS. Note the interesting 3.5" drive layout: 5 of them are in a column, and the remaining 2 (along with the SSD) are installed near the LSI controller[/caption]

[caption id="attachment_30840" align="alignnone" width="630"]<a href="http://pivotallabs.com/wordpress/wp-content/uploads/2014/10/freenas_back.jpg"><img src="http://pivotallabs.com/wordpress/wp-content/uploads/2014/10/freenas_back-630x472.jpg" alt="For easier installation of the power supply, we recommend removing the retaining screw of the controller card (it&#039;s not needed). Note that in the photo the screw has already been removed." width="630" height="472" class="size-large wp-image-30840" /></a> For easier installation of the power supply, we recommend removing the retaining screw of the controller card (it's not needed). Note that in the photo the screw has already been removed.[/caption]

The assembly is straightforward.

* Do not plug a 4-pin 12V DC power into connector J1&mdash;it's meant as an *alternative* power source when the 24-pin ATX power is not in use
* Use a consistent system when plugging in SATA data connectors. The LSI card has 2 &times; SF-8087 connectors, each of which fan out to 4 SATA connectors. We counted the drives from the top, so the topmost drive was connected to SF-8087 port 1 SATA cable 1, the second topmost drive was connected to SF-8087 port 1 SATA cable 2, etc&hellip; We connected the SSD to SF-8087 port 2 cable 4 (i.e. the final cable).

### 3. Power On
There are two caveats to the initial power on:

* the unit's power draw is so low that the Corsair power supply's fan will **not** turn on
* make sure your VGA monitor is turned on and working (ours wasn't)

### 4. Installing FreeNAS (OS X)
We download the USB image from [here](http://www.freenas.org/download-freenas-release.html). We follow the **OS X instructions** from [the manual](http://web.freenas.org/images/resources/freenas9.2.1/freenas9.2.1_guide.pdf); Windows, Linux, and FreeBSD users should consult the manual for their respective operating system.

#### 4.1 OS X: Destroy USB drive's Second GPT Table
If you have previously-formatted your USB drive with [GPT](http://en.wikipedia.org/wiki/GUID_Partition_Table) (not MBR) partitioning, you will need to wipe the second GPT table as described [here](https://forums.freenas.org/index.php?threads/freenas-8-3-0-root-mount-error.10270/). These are the commands we used. Your commands will be similar, but the sector numbers will be different. Be cautious.

```
 # use diskutil list to find the device name of our (inserted USB)
diskutil list
 # in this case it's "/dev/disk2"
diskutil info /dev/disk2
 # "Total Size:  16.0 GB (16008609792 Bytes) (exactly 31266816 512-Byte-Units)"
 # Determine the beginning of the final 8 blocks (512-byte blocks, final 4kB):
 # 31266816 - 8 = 31266808
 # wipe the last 4k bytes
sudo dd if=/dev/zero of=/dev/disk2 bs=512 oseek=31266808
```

#### 4.2 OS X: Create FreeNAS USB Image
Per the FreeNAS user manual:

```
cd ~/Downloads
xzcat FreeNAS-9.2.1.8-RELEASE-x64.img.xz > FreeNAS-9.2.1.8-RELEASE-x64.img
 # use diskutil list to find the device name of our (inserted USB)
diskutil list
 # in this case it's "/dev/disk2"
sudo dd if=FreeNAS-9.2.1.8-RELEASE-x64.img of=/dev/disk2 bs=64k
```
### 5. Boot FreeNAS
We do the following:

* place the USB key in one of the black USB 2 slots, *not* one of the blue USB 3 slots (USB 3.0 support is available if needed, check the [FreeNAS User Guide](http://web.freenas.org/images/resources/freenas9.2.1/freenas9.2.1_guide.pdf) for more information.
* connect an ethernet cable to the ethernet port that is closest to the blue USB slots
* turn on the machine: it boots from the USB key without needing modified BIOS settings

We see the following screen:

[caption id="attachment_30843" align="alignnone" width="630"]<a href="http://pivotallabs.com/wordpress/wp-content/uploads/2014/10/freenas_boot_screen.png"><img src="http://pivotallabs.com/wordpress/wp-content/uploads/2014/10/freenas_boot_screen-630x350.png" alt="The FreeNAS console. Many basic administration tasks can be performed here, mostly related to configuring the network. As our DHCP server has supplied network connectivity to the FreeNAS, we are able to configure it via the richer web interface" width="630" height="350" class="size-large wp-image-30843" /></a> The FreeNAS console. Many basic administration tasks can be performed here, mostly related to configuring the network. As our DHCP server has supplied network connectivity to the FreeNAS, we are able to configure it via the richer web interface[/caption]


### 6. Configuring FreeNAS
We log into our NAS box via our browser: [http://nas.nono.com](http://nas.nono.com) (we have previously created a DNS entry (nas.nono.com), assigned an IP address (10.9.9.80), determined the NAS's ethernet MAC address, and entered that information into our DHCP server's configuration).

#### 6.1 Our first task: set the root password.
[caption id="attachment_30844" align="alignnone" width="522"]<a href="http://pivotallabs.com/wordpress/wp-content/uploads/2014/10/set_root_password.png"><img src="http://pivotallabs.com/wordpress/wp-content/uploads/2014/10/set_root_password.png" alt="Before anything else, we must first set our root password" width="522" height="346" class="size-full wp-image-30844" /></a> Before anything else, we must first set our root password[/caption]

#### 6.2 Basic Settings
* click the **System** icon
* **Settings &rarr; General**
	* Protocol: **HTTPS**
	* click **Save**

We are redirected to an HTTPS connection with a self-signed cert. We click through the warnings.

* click the **System** icon
* **System Information &rarr; Hostname**
	* click **Edit**
	* Hostname: **nas.nono.com**
	* click **OK**

We enable ssh in order to allow us to install the disk benchmarking package ([bonnie++](http://www.coker.com.au/bonnie++/)). We enable AFP, for that will be our primary filesharing protocol. We also enable iSCSI for our ESXi host. We enable CIFS for good measure (we don't have Windows clients, but we may in the future).

* click the **Services** icon
	* click the **SSH** slider to turn it on
		* click the wrench next to the **SSH** slider.
		* check **Login as Root with password**
		* click **OK**
	* click the **AFP** slider to turn it on
	* click the **CIFS** slider to turn it on
	* click the **iSCSI** slider to turn it on

#### 6.3 Create ZFS Volume

We create one big volume. We choose ZFS's RAID-Z2 <sup>[[2]](#raid6)</sup> :

* click the **Storage Icon** icon
	* click **Active Volumes** tab
	* click **ZFS Volume Manager**
		* Volume name **Tank**
		* under **Available disks**, click **+** next to **1 - 4.0TB (7 drives, show)** (we are ignoring the 512GB SSD for the time being)
		* Volume Layout: **RaidZ2** (ignore the *non-optimal*  <sup>[[3]](#non_optimal)</sup> warning)
		* click **Add Volume**

### 7. Enable Filesharing

#### 7.1 Create User
We create user 'cunnie' for sharing:

* From the left hand navbar: **Account &rarr; Users &rarr; Add User**
	* Username: **cunnie**
	* Full Name: **Brian Cunnie**
	* Password: ***some-password-here***
	* Password confirmation: ***some-password-here***
	* click **OK**

#### 7.2 Create Directory
```
ssh root@nas.nono.com
mkdir /mnt/tank/big
chmod 1777 !$
exit
```

#### 7.3 Create Share
* Click the **Sharing** icon
* select **Apple (AFP))**
* click **Add Apple (AFP) Share**
	* Name: **big**
	* Path: **/mnt/tank/big**
	* Allow List: **cunnie**
	* Time Machine: **checked**
	* click **OK**
	* click **Yes** (enable this service)

#### 7.4 Access Share from OS X Machine

* switch to finder
* press **cmd-k** to bring up *Connect to Server* dialog
	* Server Address: **afp://nas.nono.com**
	* click **Connect**
* Name: **Brian Cunnie**
* Password: **some-password-here**

### 8. Benchmarking FreeNAS
We use [bonnie++](http://www.coker.com.au/bonnie++/) to benchmark our machine for the following reasons:

1. it's a venerable benchmark
2. it allows easy comparison to Calomel.org's bonnie++ benchmarks

We use a file size of 80GiB to eliminate the RAM cache (ARC) skewing the numbers&mdash;we are measuring disk performance, not RAM performance.

```
ssh root@nas.nono.com
 # we remount the root filesystem as read-write so that we
 # can install bonnie++
mount -o rw /
pkg_add -r bonnie++
 # we add root to sudoers because that will allow us
 # to run bonnie++ as a _non-root_ user, which it requires.
cat >> /usr/local/etc/sudoers <<EOF
  root  ALL=(ALL) NOPASSWD: ALL
EOF
 # create a temporary directory to hold bonnie++'s
 # scratch files
mkdir /mnt/tank/tmp
chmod 1777 !$
 # 9 series of runs, 8 jobs in parallel, median value
 # kick off 8 jobs (8 cores) to minimize CPU-bottleneck
foreach I (0 1 2 3 4 5 6 7 8)
  ( sudo -u nobody bonnie++ -m "RAIDZ2_8C" -r 8192 -s 81920 -d /mnt/tank/tmp/ -f -b -n 1; date ) >> /mnt/tank/tmp/bonnie.txt &
  ( sudo -u nobody bonnie++ -m "RAIDZ2_8C" -r 8192 -s 81920 -d /mnt/tank/tmp/ -f -b -n 1; date ) >> /mnt/tank/tmp/bonnie.txt &
  ( sudo -u nobody bonnie++ -m "RAIDZ2_8C" -r 8192 -s 81920 -d /mnt/tank/tmp/ -f -b -n 1; date ) >> /mnt/tank/tmp/bonnie.txt &
  ( sudo -u nobody bonnie++ -m "RAIDZ2_8C" -r 8192 -s 81920 -d /mnt/tank/tmp/ -f -b -n 1; date ) >> /mnt/tank/tmp/bonnie.txt &
  ( sudo -u nobody bonnie++ -m "RAIDZ2_8C" -r 8192 -s 81920 -d /mnt/tank/tmp/ -f -b -n 1; date ) >> /mnt/tank/tmp/bonnie.txt &
  ( sudo -u nobody bonnie++ -m "RAIDZ2_8C" -r 8192 -s 81920 -d /mnt/tank/tmp/ -f -b -n 1; date ) >> /mnt/tank/tmp/bonnie.txt &
  ( sudo -u nobody bonnie++ -m "RAIDZ2_8C" -r 8192 -s 81920 -d /mnt/tank/tmp/ -f -b -n 1; date ) >> /mnt/tank/tmp/bonnie.txt &
  ( sudo -u nobody bonnie++ -m "RAIDZ2_8C" -r 8192 -s 81920 -d /mnt/tank/tmp/ -f -b -n 1; date ) >> /mnt/tank/tmp/bonnie.txt &
  wait
  sleep 60
end
#
```
The raw bonnie++ output is available on [GitHub](https://github.com/cunnie/freenas_benchmarks/blob/master/RAIDZ2_8C.txt). The summary (median scores): (w=993MB/s, r=1882MB/s, IOPS=884)

### 9. Summary
#### 9.1 IOPS could be improved
The IOPS (~884) are respectable. Although well more than four times as fast as a 15k RPM SAS Drive ([~175-210 IOPS](http://en.wikipedia.org/wiki/IOPS#Examples)), it's still much lower than a high-end SSD offers (e.g. an Intel X25-M G2 (MLC) posts ~8,600). We feel that using the SSD as a second-level cache could improve our numbers dramatically.

#### 9.2 No SSD
We never put the SSD to use. We plan to use the SSD as both a [L2ARC](https://blogs.oracle.com/brendan/entry/test) (ZFS read cache) and a [ZIL SLOG](https://pthree.org/2012/12/06/zfs-administration-part-iii-the-zfs-intent-log/) (a ZFS write cache for synchronous writes).

#### 9.3 Gigabit Bottleneck
Our NAS's performance is *severely limited by the throughput of its gigabit interface* on its sequential reads and writes. Our ethernet interface is limited to [~111 MB/s](http://www.tomshardware.com/reviews/gigabit-ethernet-bandwidth,2321-7.html), but our sequential reads can reach almost seventeen times that (1882MB/s).

We can partly address that by using [LACP](http://en.wikipedia.org/wiki/Link_aggregation) (aggregating the throughput of the 4 available ethernet interfaces).

#### 9.4 Noise
The fans in the case were noiser than expected, Not clicking or tapping, but a discernible hum.

#### 9.5 Heat
The system runs cool. With a room temperature of 23.3&deg;C (74&deg; Fahrenheit), these are the readings we recorded after the machine being powered on for 12 hours:

* CPU: **30&deg;C**
* System: **32&deg;C**
* Peripheral: **31&deg;C**
* DIMMA1: **30&deg;C**
* DIMMA2: **32&deg;C**
* DIMMB1: **33&deg;C**
* DIMMB2: **34&deg;C**

No component is warmer than body temperature. We are especially impressed with the low CPU temperature, doubly so that it's passively cooled.

#### 9.6 No Hot Swap
It would be nice if the system had a hot-swap feature. It doesn't. In the event we need to replace a drive, we'll be powering the system down.

#### 9.7 Pool Alignment
FreeNAS does the right thing: it creates 4kB-aligned pools by default (instead of a 512B-aligned pools). This *should* be  more efficient, though results vary. See Calomel.org's section, [Performance of 512b versus 4K aligned pools](https://calomel.org/zfs_raid_speed_capacity.html) for an in-depth discussion and benchmarks.

### 10. More Extensive Benchmarking
In our follow-on [post](http://pivotallabs.com/high-performing-mid-range-nas-server-part-2-performance-tuning-iscsi/), we tune our ZFS fileserver for optimal iSCSI performance.

---
### Footnotes

<a name="cpu_bound"><sup>1</sup></a> These numbers are not terribly exact. To overcome being artificially limited by the CPU, we were forced to run 8 benchmarks in parallel. This had two serious shortcomings:

1. The individual benchmarks weren't synchronized&mdash;benchmarks finished as much as ten seconds apart.  While one benchmark was finishing up its rewriting portion, another had already moved on to the reading portion, causing a distortion in the usage pattern.
2. The numbers weren't derived by summing the numbers from a single run of 8 benchmarks. Instead, *all* the benchmark results were aggregated, and the median 8 values were taken and summed.

For those interested in the raw benchmark data, they can be seen [here](https://github.com/cunnie/freenas_benchmarks/blob/master/RAIDZ2_8C.txt).

<a name="raid6"><sup>2</sup></a> We feel that [double-parity RAID](http://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_6) is a safer approach than single-parity (e.g. RAID 5). Adam Leventhal, in his [article for the ACM](http://queue.acm.org/detail.cfm?id=1670144), describes the challenges that large capacity disks pose to a RAID 5 solution. A [NetApp paper](http://synergy-ds.com/netapp/wp-7005.pdf) states, "&hellip; in the event of a drive failure, utilizing a
SATA RAID 5 group (using 2TB disk drives) can mean
a *33.28% chance of data loss per year*" (italics ours).

<a name="non_optimal"><sup>3</sup></a> We aren't concerned about a *non-optimal* configuration (i.e. the number of disks (less parity) should optimally be a power of 2)&mdash;we have reservations about the statement, "the number of disks should be a power of 2 for best performance". A [serverfault post](http://serverfault.com/questions/365903/why-does-raid5-with-an-odd-number-of-data-disks-have-poor-write-performance) states, "As a general rule, the performance boost of adding an additional spinddle [*sic*] will exceed the performance cost of having a sub-optimal drive count". Also, we are enabling compression on the ZFS volume, which means that the stripe size will be variable rather than a power of 2 (we are guessing; we may be wrong), which de-couples the stripe size from the disks' block size.

### Acknowledgements

Calomel.org has one of the [most comprehensive set of ZFS benchmarks](https://calomel.org/zfs_raid_speed_capacity.html) and good advice for maximizing the performance of ZFS, some of it not obvious (e.g. the importance of a good controller)

# A High-performing Mid-range NAS Server
## Part 2: Performance Tuning for iSCSI
This blog post describes how we tuned and benchmarked our FreeNAS fileserver for optimal iSCSI performance.

For most workloads (except ones that are extremely sequential-read intensive) we recommend using L2ARC, SLOG, and the experimental iSCSI kernel target.

Of particular interest is the experimental iSCSI driver, which increased **our IOPS 334%** and increased our sequential write performance to its maximum, **112MB/s** (capped by the speed of our ethernet connection). On the downside, there was a **45%** *decrease* in sequential read speed.

***[2014-11-6 We have added a third round of benchmarks]***

Using an L2ARC also improved performance (IOPS increased 46%, sequential write improved 13%, and sequential read *decreased* 4%).

We also experimented with three ZFS sysctl variables, but they were a mixed bag (they improved some metrics to the detriment of others).

Here is the summary of our results in a chart format:

[caption id="attachment_31381" align="alignnone" width="630"]<a href="https://lh3.googleusercontent.com/-u5wTbZ_eWx8/VFuRWFijShI/AAAAAAAAJ0s/D-Jh625S8N8/w704-h569-no/FreeNAS%2B9.2.1.8%2BBenchmarks.png"><img src="https://lh3.googleusercontent.com/-u5wTbZ_eWx8/VFuRWFijShI/AAAAAAAAJ0s/D-Jh625S8N8/w704-h569-no/FreeNAS%2B9.2.1.8%2BBenchmarks.png" alt="Summary of Benchmark results. Note that Sequential Write and Read use the left axis (MB/s), and that IOPS is measured against the logarithmic right axis." width="704" height="569" class="size-large wp-image-31381" /></a> Summary of Benchmark results. Note that Sequential Write and Read use the left axis (MB/s), and that IOPS is measured against the logarithmic right axis. Higher is better.[/caption]

There is no optimal configuration; rather, FreeNAS can be configured to suit a particular workload:

* to maximize sequential write performance, use the experimental kernel iSCSI target and an L2ARC
* to maximize sequential read performance, use the default userland iSCSI target and no L2ARC
* to maximize IOPS, use the experimental kernel iSCSI target, L2ARC, enable prefetching tunable, and aggressively modify two sysctl variables.

### 0. Background
#### 0.0 Hardware Configuration
We describe the hardware and software configuration in a previous post, [A High-performing Mid-range NAS Server](http://pivotallabs.com/high-performing-mid-range-nas-server/). Highlights:

* FreeNAS 9.2.1.8
* Intel 8-core Avoton C2750
* 32GiB RAM
* 7 x 4TB disks
* RAIDZ2
* 512GB SSD (unused)
* 4 x 1Gbe

#### 0.1 Metrics and Tools
We use [bonnie++](http://www.coker.com.au/bonnie++/) to measure disk performance. `bonnie++` produces many performance metrics (e.g. "Sequential Output Rewrite Latency"); we focus on three of them:

1. Sequential Write ("Sequential Output Block")
2. Sequential Read ("Sequential Input Block")
3. IOPS ("Random Seeks")

We use an 80G file for our `bonnie++` tests. We store the raw output of our benchmarks in a GitHub [repo](https://github.com/cunnie/freenas_benchmarks).

#### 0.2 iSCSI Setup
Our FreeNAS server provides storage (data store) via iSCSI to VMs running on our ESXi server. This post does not cover setting up iSCSI and accessing it from ESXi; however, Steve Erdman has written such a blog post, "[Connecting FreeNAS 9.2 iSCSI to ESXi 5.5 Hypervisor and performing VM Guest Backups](http://www.erdmanor.com/blog/connecting-freenas-9-2-iscsi-esxi-5-5-hypervisor-performing-vm-guest-backups/)"

#### 0.3 iSCSI and the exclusion of Native Performance
Although we have measured the native performance of our NAS (i.e. we have run `bonnie++` directly on our NAS, bypassing the limitation of our 1Gbe interface), we don't find those numbers terribly meaningful. We are interested in real-world performance of VMs whose data store is on the NAS and which is mounted via iSCSI.

#### 0.4 Untuned Numbers and Upper Bounds
We want to know what our upper bounds are; this will be important as we progress in our tuning&mdash;once we hit an theoretical maximum for a given metric, there's no point in additional tuning for that metric.

The 1Gb ethernet interface places a hard limit on our sequential read and write performance: 111MB/s.

For comparison we have added the performance of our external USB hard drive (the performance numbers are from a VM whose data store resided on a USB hard drive). Note that the external USB hard drive is not limited by gigabit ethernet throughput, and thus is able to post a Sequential Read benchmark that exceeds the theoretical maximum.

<table>
<tr>
<th></th><th>Sequential Write<br />(MB/s)<br /><i>(higher is better)</i></th><th>Sequential Read<br />(MB/s)<br /><i>(higher is better)</i></th><th>IOPS<br /><i>(higher is better)</i></th>
</tr><tr>
<th>Untuned<br /></th><td>59</td><td>74</td><td>99.8</td>
</tr><tr>
<th>Theoretical<br />Maximum</th><td>111</td><td>111</td><td></td>
</tr><tr>
<th>External<br />4TB USB3<br />7200 RPM</th><td>33</td><td>159</td><td>121.8</td>
</tr>
</table>

The raw benchmark data is available [here](https://github.com/cunnie/freenas_benchmarks/blob/master/iSCSI_80G).
### 1. L2ARC and ZIL SLOG
[L2ARC](https://blogs.oracle.com/brendan/entry/test) is ZFS's secondary read cache (ARC, the primary cache, is RAM-based).

Using an L2ARC can increase our IOPS "[8.4x faster than with disks alone.](https://blogs.oracle.com/brendan/entry/test)"

[ZIL (ZFS Intent Log) SLOG (Separate Intent Log)](https://pthree.org/2012/12/06/zfs-administration-part-iii-the-zfs-intent-log/) is a "&hellip; separate logging device that caches the synchronous parts of the ZIL before flushing them to slower disk".

Typically an SSD drive is used as secondary cache; we use a [Crucial MX100 512GB SSD](http://www.crucial.com/usa/en/ct512mx100ssd1).

We will implement L2ARC and SLOG and determine the improvement.
#### 1.0 Determine Size of L2ARC (190GB)
L2ARC sizing is dependent upon available RAM (L2ARC exacts a price in RAM), available disk (we have a 512GB SSD), and average buffer size (the L2ARC requires [40bytes of RAM](http://osdir.com/ml/zfs-discuss/2014-05/msg00050.html) for each buffer. Buffer sizes vary).

We first determine the amount of RAM we have available:

```
ssh root@nas.nono.com
 # determine the amount of RAM available
top -d 1 | head -6 | tail -3
Mem: 250M Active, 3334M Inact, 26G Wired, 236M Cache, 467M Buf, 929M Free
  ARC: 24G Total, 2073M MFU, 20G MRU, 120K Anon, 1303M Header, 574M Other
  Swap: 14G Total, 23M Used, 14G Free
```
We see we have **5GiB** RAM at our disposal for our L2ARC (32GiB total - 1GiB Operating System - 26GiB "Wired" = 5GiB L2ARC).

We arrive at our L2ARC sizing experimentally: we note that when we use a 200GB L2ARC, we see that ~200MiB of swap is used. We prefer not to use swap at all, so we know that we want to reduce our L2ARC RAM footprint by 200MiB (i.e. instead of 5GiB RAM, we only want to use 4.8GiB). We find that a 190GB L2ARC meets that need.

For our configuration, we need **1GiB of RAM for every 38GB of L2ARC**
#### 1.1 Determine Size of SLOG (12GB)
We use this [forum post](https://forums.freenas.org/index.php?threads/some-insights-into-slog-zil-with-zfs-on-freenas.13633/) to determine the size of our SLOG:

* The SLOG "must be large enough to hold a minimum of two transaction groups"
* A transaction group is sized by either RAM or time, i.e. "In FreeNAS, the default size is 1/8th your system's memory" or 5 seconds
* Based on 32GiB RAM, our transaction group is 4GiB
* We will triple that amount to 12GB and use that to size our SLOG (i.e. our SLOG will be able to store 3 transaction groups)

We note that we most likely over-spec'ed our SLOG by a factor of 12, "[I can't imagine what sort of workload you would need to get your ZIL north of 1 GB of used space](http://osdir.com/ml/zfs-discuss/2014-05/msg00050.html)"

#### 1.1 Create L2ARC Partition
We use a combination of `sysctl` and `diskinfo` to determine our disks:

```
foreach DISK ( `sysctl -b kern.disks` )
  diskinfo $DISK
end
da8	512	16008609792	31266816	0	0	1946	255	63
da7	512	4000787030016	7814037168	4096	0	486401	255	63
da6	512	4000787030016	7814037168	4096	0	486401	255	63
da5	512	4000787030016	7814037168	4096	0	486401	255	63
da4	512	512110190592	1000215216	4096	0	62260	255	63
da3	512	4000787030016	7814037168	4096	0	486401	255	63
da2	512	4000787030016	7814037168	4096	0	486401	255	63
da1	512	4000787030016	7814037168	4096	0	486401	255	63
da0	512	4000787030016	7814037168	4096	0	486401	255	63
```
We see that **da4** is our 512G SSD (and **da8** is our 16GB bootable USB stick and the remaining disks are our 4TB Seagates which make up our RAID Z2).

We use `gpart` to initialize **da4**. Then we create a 190GB partition which we align on 4kB boundaries (`-a 4k`):

```
gpart create -s GPT da4
  da4 created
gpart add -s 190G -t freebsd-zfs -a 4k da4
  da4p1 added
```

Create a 12GB SLOG:

```
gpart add -s 12G -t freebsd-zfs -a 4k da4
```

#### 1.2 Add L2ARC to ZFS Pool
We add our new L2ARC and SLOG partitions:

```
zpool add tank cache da4p1
zpool add tank log da4p2
zpool status
```

#### 1.3 L2ARC Results
We perform 7 runs and take the median values for each metric (e.g. Sequential Write). The L2ARC provides us a 14% increase in write speed, a 4% *decrease* in read speed, and a 46% increase in IOPS.

<table>
<tr>
<th></th><th>Sequential Write<br />(MB/s)<br /><i>(higher is better)</i></th><th>Sequential Read<br />(MB/s)<br /><i>(higher is better)</i></th><th>IOPS<br /><i>(higher is better)</i></th>
</tr><tr>
<th>Untuned<br /></th><td>59</td><td>74</td><td>99.8</td>
</tr><tr>
<th>200G L2ARC</th><td>67</td><td>71</td><td>145.7</td>
</tr><tr>
<th>Theoretical<br />Maximum</th><td>111</td><td>111</td><td></td>
</tr>
</table>

The raw benchmark data can be seen [here](https://github.com/cunnie/freenas_benchmarks/blob/master/iSCSI_L2_80G.txt).
### 2. Experimental Kernel-based iSCSI
FreeNAS 9.2.1.6 includes an [experimental kernel-based iSCSI target](http://download.freenas.org/9.2.1.6/RELEASE/ReleaseNotes). We enable the target and reboot our machine.

#### 2.0 Configure Experimental iSCSI Target
* We browse to our FreeNAS server: [https://nas.nono.com](https://nas.nono.com)
* log in
* click the **Services** icon at the top
* click the "wrench" icon

[caption id="attachment_31317" align="alignnone" width="564"]<a href="http://pivotallabs.com/wordpress/wp-content/uploads/2014/10/wrench_icon.png"><img src="http://pivotallabs.com/wordpress/wp-content/uploads/2014/10/wrench_icon.png" alt="To modify the iSCSI services settings and enable the experimental kernel driver, click the wrench icon" width="564" height="451" class="size-full wp-image-31317" /></a> To modify the iSCSI services settings and enable the experimental kernel driver, click the wrench icon[/caption]

* check the **Enable experimental target** checkbox

[caption id="attachment_31318" align="alignnone" width="630"]<a href="http://pivotallabs.com/wordpress/wp-content/uploads/2014/10/Kernel-Target.png"><img src="http://pivotallabs.com/wordpress/wp-content/uploads/2014/10/Kernel-Target-630x410.png" alt="Check the &quot;Enable experimental target&quot; to activate the kernel-based iSCSI target " width="630" height="410" class="size-large wp-image-31318" /></a> Check the "Enable experimental target" to activate the kernel-based iSCSI target [/caption]

* click **Save**
* we see a message: **Enabling experimental target requires a reboot. Do you want to proceed?**. Click **Yes**
* our FreeNAS server reboots

#### 2.1 Re-enable iSCSI
After reboot we notice that our iSCSI service has been disabled (bug?). We re-enable it:

* We browse to our FreeNAS server: [https://nas.nono.com](https://nas.nono.com)
* log in
* click the **Services** icon at the top
* click the **iSCSI** slider so it turns on

#### 2.2 Kernel iSCSI Results
We perform 9 runs and take the median values for each metric (e.g. Sequential Write). The experimental iSCSI target provides us a 67% increase in write speed (*hitting the theoretical limit*), a 45% *decrease* in read speed, and a 334% increase in IOPS.

The decrease in read speed is curious; we hope it's a FreeBSD bug that has been addressed in 10.0.

<table>
<tr>
<th></th><th>Sequential Write<br />(MB/s)<br /><i>(higher is better)</i></th><th>Sequential Read<br />(MB/s)<br /><i>(higher is better)</i></th><th>IOPS<br /><i>(higher is better)</i></th>
</tr><tr>
<th>Untuned<br /></th><td>59</td><td>74</td><td>99.8</td>
</tr><tr>
<th>200G L2ARC</th><td>67</td><td>71</td><td>145.7</td>
</tr><tr>
<th>L2ARC +<br />Experimental<br />kernel-based<br />iSCSI target</th><td>112</td><td>39</td><td>633.0</td>
</tr><tr>
<th>Theoretical<br />Maximum</th><td>111</td><td>111</td><td></td>
</tr>
</table>

The raw benchmark data is available [here](https://github.com/cunnie/freenas_benchmarks/blob/master/iSCSI_K_L2_80G.txt).
### 3. L2ARC Tuning
We want to aggressively use the L2ARC. The [FreeBSD ZFS Tuning Guide](https://wiki.freebsd.org/ZFSTuningGuide) suggests focusing on 3 tunables:

1. `vfs.zfs.l2arc_write_boost`
2. `vfs.zfs.l2arc_write_max`
3. `vfs.zfs.l2arc_noprefetch`

We ssh into our NAS to determine the current settings:

```
ssh root@nas.nono.com
sysctl -a | egrep -i "l2arc_write_max|l2arc_write_boost|l2arc_noprefetch"
  vfs.zfs.l2arc_noprefetch: 1
  vfs.zfs.l2arc_write_boost: 8388608
  vfs.zfs.l2arc_write_max: 8388608
```
#### 3.0 l2arc_write_max and l2arc_write_boost
The FreeBSD ZFS Tuning Guide states, "Modern L2ARC devices (SSDs) can handle an order of magnitude higher than the default". We decide to increase the amount from 8 MB/s to 201 MB/s (we increase it 25 times):

* on the left hand navbar, navigate to **System &rarr; Sysctls &rarr; Add Sysctl**
    * Variable: **vfs.zfs.l2arc_write_max**
    * Value: **201001001**
    * Comment: **201 MB/s**
    * click **OK**
* click **Add Sysctl**
    * Variable: **vfs.zfs.l2arc_write_boost**
    * Value: **201001001**
    * Comment: **201 MB/s**
    * click **OK**


#### 3.1 l2arc_no_prefetch
`vfs.zfs.l2arc_noprefetch` is interesting: it allows us to cache streaming data. Unfortunately, it must be set *before* the ZFS pool is imported (i.e. it can't be set in `/etc/sysctl.conf`; it must be set in `/boot/loader.conf`). That means we must set this variable as a *tunable* rather than as a *sysctl*:

* on the left hand navbar, navigate to **System &rarr; Tunables &rarr; Add Tunable**
    * Variable: **vfs.zfs.l2arc_noprefetch**
    * Value: **0**
    * Comment: **disable no_prefetch (enable prefetch)**
    * click **OK**

Reboot (browse the lefthand navbar of the web interface and click **Reboot**). Click **Reboot** when prompted.

#### 3.2 L2ARC Tuning Results (IOPS improves)
We run our tests and note the following results:

* Sequential write performance drops 35% from 112 MB/s to 73
* Sequential read performance *increases* 35% from 39 MB/s to 53
* IOPS performance more than doubles (120%) from 633 to 1392.

<table>
<tr>
<th></th><th>Sequential Write<br />(MB/s)<br /><i>(higher is better)</i></th><th>Sequential Read<br />(MB/s)<br /><i>(higher is better)</i></th><th>IOPS<br /><i>(higher is better)</i></th>
</tr><tr>
<th>Untuned<br /></th><td>59</td><td>74</td><td>99.8</td>
</tr><tr>
<th>200G L2ARC</th><td>67</td><td>71</td><td>145.7</td>
</tr><tr>
<th>L2ARC +<br />Experimental<br />kernel-based<br />iSCSI target</th><td>112</td><td>39</td><td>633.0</td>
</tr><tr>
<th>L2ARC +<br />Experimental<br />kernel-based<br />iSCSI target<br />+ tuning</th><td>73</td><td>53</td><td>1392</td>
</tr>
</table>

The raw benchmark data can be seen [here](https://github.com/cunnie/freenas_benchmarks/blob/master/iSCSI_K_L2_80G_tune_prefetch.txt). The results are a mixed bag&mdash;we like the improved read and IOPS performance, but we're dismayed by the drop in the write performance, which is our most important metric (our workload is write-intensive)

### 4. L2ARC Tuning Round 2 (worse all-around)
We disable pre-fetch.

* vfs.zfs.l2arc_noprefetch=1

This requires us to reboot our NAS to take effect.

We also use a more rigorous approach to setting the two remaining variables by determining the write throughput of the SSD by copying a large file to a raw partition; we determine that the SSD can sustain a write throughput of 193 MB/s:

```
ssh root@nas.nono.com
 # add a 20G for benchmark testing
gpart add -s 20G -t freebsd-zfs -a 4k da4
  da4p3 added
 # let's copy an 11G file to benchmark the SSD raw write speed
dd if=/mnt/tank/big/iso/ML_2012-08-27_18-51.i386.hfs.dmg of=/dev/da4p3 bs=1024k
  dd: /dev/da4p3: Invalid argument
  11460+1 records in
  11460+0 records out
  12016680960 bytes transferred in 62.082430 secs (193560094 bytes/sec)
 # our SSD write throughput is 193 MB/s
 # remove the unneeded device
gpart delete -i 3 da4
```

* vfs.zfs.l2arc_write_max=193560094
* vfs.zfs.l2arc_write_boost=193560094

<table>
<tr>
<th></th><th>Sequential Write<br />(MB/s)<br /><i>(higher is better)</i></th><th>Sequential Read<br />(MB/s)<br /><i>(higher is better)</i></th><th>IOPS<br /><i>(higher is better)</i></th>
</tr><tr>
<th>Untuned<br /></th><td>59</td><td>74</td><td>99.8</td>
</tr><tr>
<th>200G L2ARC</th><td>67</td><td>71</td><td>145.7</td>
</tr><tr>
<th>L2ARC +<br />Experimental<br />kernel-based<br />iSCSI target</th><td>112</td><td>39</td><td>633.0</td>
</tr><tr>
<th>L2ARC +<br />Experimental<br />kernel-based<br />iSCSI target<br />+ tuning</th><td>73</td><td>53</td><td>1392</td>
</tr><tr>
<th>L2ARC +<br />Experimental<br />kernel-based<br />iSCSI target<br />+ tuning<br />Round 2</th><td>65</td><td>35</td><td>1178</td>
</tr>
</table>

This has been a step backwards for us&mdash;every metric performed worse. We suspect that disabling pre-fetch was a mistake.

The raw data is available [here](https://github.com/cunnie/freenas_benchmarks/blob/master/iSCSI_K_L2_80G_tune.txt).

### 5. L2ARC Tuning Round 3
We re-enable pre-fetch:

* vfs.zfs.l2arc_noprefetch=0

We drop the value of the remaining sysctls from 193 MB/s to 81 MB/s (the FreeBSD Tuning Guide suggested an order-of-magnitude increase from the default of 8 MB/s; we increase by 10&times; (one order of magnitude) rather than by 23&times;):

* vfs.zfs.l2arc_write_max=81920000
* vfs.zfs.l2arc_write_boost=81920000

We run our benchmark again:

<table>
<tr>
<th></th><th>Sequential Write<br />(MB/s)<br /><i>(higher is better)</i></th><th>Sequential Read<br />(MB/s)<br /><i>(higher is better)</i></th><th>IOPS<br /><i>(higher is better)</i></th>
</tr><tr>
<th>Untuned<br /></th><td>59</td><td>74</td><td>99.8</td>
</tr><tr>
<th>200G L2ARC</th><td>67</td><td>71</td><td>145.7</td>
</tr><tr>
<th>L2ARC +<br />Experimental<br />kernel-based<br />iSCSI target</th><td>112</td><td>39</td><td>633.0</td>
</tr><tr>
<th>L2ARC +<br />Experimental<br />kernel-based<br />iSCSI target<br />+ tuning</th><td>73</td><td>53</td><td>1392</td>
</tr><tr>
<th>L2ARC +<br />Experimental<br />kernel-based<br />iSCSI target<br />+ tuning<br />Round 2</th><td>65</td><td>35</td><td>1178</td>
</tr><tr>
<th>L2ARC +<br />Experimental<br />kernel-based<br />iSCSI target<br />+ tuning<br />Round 3</th><td>97</td><td>36</td><td>1633</td>
</tr>
</table>

This configuration has achieved the highest IOPS score of any we've benchmarked&mdash;a sixteen-fold increase from the untuned configuration. It approaches the IOPS of an SSD (~8,600).

This also posts the second-highest sequential write throughput, a quite-respectable 97 MB/s.

The sequential read is disappointing&mdash;the only good thing to say that it's not the absolute worst (but it is the second-worst). To re-iterate, we hope that this is a kernel iSCSI bug that's addressed in a future release of FreeBSD.

The raw data is available [here](https://github.com/cunnie/freenas_benchmarks/blob/master/iSCSI_K_L2_80G_tune_prefetch_2.txt).

### 6. Conclusion
#### 6.0 No "Magic Bullet"
There's no "magic bullet" to ZFS performance tuning that improves *all* metrics.

For most workloads (except ones that are extremely sequential-read intensive) we recommend using L2ARC, SLOG, and the experimental iSCSI kernel target.

#### 6.1 Our Configuration
We chose the final configuration (best IOPS, second-best sequential write, second-worst sequential read) for our setup. Our workload is write-intensive and IOPS-intensive.

#### 6.2 Test Shortcomings

* Subtle inconsistencies: some tests were run with a 200GB L2ARC and no SLOG, and later tests were run with a 190GB L2ARC and a 12GB SLOG
* Not completely-dedicated NAS: the NAS was not completely dedicated to running benchmarks&mdash;it was also acting as a Time Machine backup during the majority of the testing. It is possible that some of the numbers would be slightly higher if it was completely dedicated
* The size of the test file (80G) was very specific: it was meant to exceed the ARC but not exceed the L2ARC. We ran three tests with file sizes *smaller* than the ARC, and the results (unsurprisingly) were excellent ([4GB file](https://github.com/cunnie/freenas_benchmarks/blob/master/iSCSI_L2_4G.txt), [8GB](https://github.com/cunnie/freenas_benchmarks/blob/master/iSCSI_L2_8G.txt),  and [16GB file](https://github.com/cunnie/freenas_benchmarks/blob/master/iSCSI_L2_16G))
* The test took almost 24 hours to run; this impeded our ability to run as many tests as we would have liked
* We would have liked to have run some of the benchmarks a second time to eliminate the possibility that our testbed changed (e.g. intense benchmarking may have caused the SSD performance to diminish towards the end of the testing)
* We would have liked to have been able to eliminate the limitation of the 1 gigabit ethernet link; it would be interesting to see the performance with a 10Gbe link
* The scope of the tests were very narrow (e.g. iSCSI-only, a very specific server hardware configuration). It would be overreaching to generalize these numbers to all ZFS fileservers or even all protocols (e.g. AFP, CIFS).
